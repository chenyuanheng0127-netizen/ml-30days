{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b7aa8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "def train_test_split(X, y, test_size=0.25):\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    n_te = int(n * test_size)\n",
    "    te, tr = idx[:n_te], idx[n_te:]\n",
    "    return X[tr], X[te], y[tr], y[te]\n",
    "\n",
    "def standardize(X):\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    std = X.std(axis=0, keepdims=True) + 1e-12\n",
    "    return (X - mu)/std, mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814b1de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=800, n_features=4, n_informative=3,\n",
    "                           class_sep=1.5, random_state=42)\n",
    "X, mu, std = standardize(X)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25)\n",
    "X_tr.shape, X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410390de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    return np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -20, 20)\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def loss_and_grad(w, X, y, l2=0.0):\n",
    "    # X: [N, d], w: [d], y: [N]\n",
    "    N = X.shape[0]\n",
    "    p = sigmoid(X @ w)\n",
    "    # 交叉熵 + L2（不惩罚偏置项）\n",
    "    l2_term = 0.5 * l2 * (w[1:] @ w[1:])\n",
    "    loss = - (y*np.log(p+1e-12) + (1-y)*np.log(1-p+1e-12)).mean() + l2_term\n",
    "    grad = (X.T @ (p - y))/N\n",
    "    grad[1:] += l2 * w[1:]\n",
    "    return loss, grad\n",
    "\n",
    "def fit_logreg(X, y, lr=0.1, epochs=300, l2=0.0):\n",
    "    Xb = add_bias(X)\n",
    "    w = np.zeros(Xb.shape[1])\n",
    "    hist = []\n",
    "    for t in range(epochs):\n",
    "        L, g = loss_and_grad(w, Xb, y, l2=l2)\n",
    "        w -= lr * g\n",
    "        if (t+1)%20==0 or t==0:\n",
    "            hist.append(L)\n",
    "    return w, np.array(hist)\n",
    "\n",
    "def predict_proba(X, w):\n",
    "    return sigmoid(add_bias(X) @ w)\n",
    "\n",
    "def predict(X, w, thresh=0.5):\n",
    "    return (predict_proba(X, w) >= thresh).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63cc38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def grad_check(X, y, l2=0.0, eps=1e-5):\n",
    "    Xb = add_bias(X)\n",
    "    w0 = np.random.randn(Xb.shape[1]) * 0.1\n",
    "    _, g = loss_and_grad(w0, Xb, y, l2=l2)\n",
    "    num_g = np.zeros_like(w0)\n",
    "    for i in range(len(w0)):\n",
    "        w1 = w0.copy(); w1[i] += eps\n",
    "        w2 = w0.copy(); w2[i] -= eps\n",
    "        L1,_ = loss_and_grad(w1, Xb, y, l2=l2)\n",
    "        L2,_ = loss_and_grad(w2, Xb, y, l2=l2)\n",
    "        num_g[i] = (L1 - L2) / (2*eps)\n",
    "    rel_err = np.linalg.norm(g - num_g) / (np.linalg.norm(g) + np.linalg.norm(num_g) + 1e-12)\n",
    "    return rel_err\n",
    "\n",
    "rel = grad_check(X_tr[:64], y_tr[:64], l2=0.1)\n",
    "print(\"gradient check relative error:\", rel)\n",
    "assert rel < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb5982",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "w, hist = fit_logreg(X_tr, y_tr, lr=0.2, epochs=400, l2=0.1)\n",
    "y_hat_tr = predict(X_tr, w); y_hat_te = predict(X_te, w)\n",
    "acc_tr = (y_hat_tr==y_tr).mean(); acc_te=(y_hat_te==y_te).mean()\n",
    "print(f\"train acc={acc_tr:.3f}, test acc={acc_te:.3f}\")\n",
    "\n",
    "plt.plot(hist); plt.title(\"Training loss\"); plt.xlabel(\"checkpoints\"); plt.ylabel(\"loss\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
